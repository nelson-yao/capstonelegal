{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import gzip # for compressing Pickeled data\n",
    "import xml.etree.ElementTree as ET\n",
    "from textblob import TextBlob\n",
    "from glob import glob\n",
    "import os.path as path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read vectorizers and classifiers from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with gzip.open('./Data/classifiers_rforest.pk.gz', 'rb') as f:\n",
    "    models = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with gzip.open('./Data/vectorizers.pk.gz', 'rb') as f:\n",
    "    vectorizers = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to read privacy policies from corpus\n",
    "Using the ACL Coling dataset from https://www.usableprivacy.org/static/data/acl-coling-2014-corpus.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def getSentencesFromXML(filename):\n",
    "\n",
    "    tree = ET.parse(filename)\n",
    " \n",
    "    # extract the website_url attribute\n",
    "    website = tree.find('.').attrib['website_url']\n",
    "    \n",
    "    # get a list of sentences from the privacy policy\n",
    "    sentences = []\n",
    "    for section in tree.getroot():\n",
    "        for subsection in section:\n",
    "            try:\n",
    "                blob = TextBlob(subsection.text)\n",
    "            except TypeError:\n",
    "                continue # ignore\n",
    "            for s in blob.sentences:\n",
    "                sentences.append(str(s))\n",
    "                \n",
    "    return website, sentences\n",
    "\n",
    "def getSentencesFromTXT(filename):\n",
    "    # get a list of sentences from the privacy policy\n",
    "    sentences = []\n",
    "\n",
    "    with open(filename, 'r') as f:\n",
    "        blob = TextBlob(f.read())\n",
    "        \n",
    "    for s in blob.sentences:\n",
    "        sentences.append(str(s))\n",
    "        \n",
    "    return sentences\n",
    "\n",
    "\n",
    "def classifySentences(vectorizers, models, sentences):\n",
    "    \n",
    "    classifiedSentences = {}\n",
    "    \n",
    "    for category in models:\n",
    "        vectorizer = vectorizers[category]\n",
    "        model = models[category]\n",
    "\n",
    "        X = vectorizer.transform(sentences)\n",
    "        predictions = model.predict(X)\n",
    "        \n",
    "        classifiedSentences[category] = []\n",
    "        \n",
    "        for i, predict in enumerate(predictions):\n",
    "            if predict == 1:\n",
    "                classifiedSentences[category].append(sentences[i])\n",
    "\n",
    "    return classifiedSentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through all files and classify each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 files processed\n",
      "\n",
      "CPU times: user 742 ms, sys: 32.5 ms, total: 775 ms\n",
      "Wall time: 774 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#filenames = glob('./Data/corpus/*.xml')\n",
    "filenames = glob('./Data/unroll.me')\n",
    "\n",
    "corpus_sentences = {}\n",
    "\n",
    "for filename in filenames:\n",
    "    if filename[-4:] == \".xml\":\n",
    "        website, sentences = getSentencesFromXML(filename)\n",
    "    else: # assume text file\n",
    "        website = path.basename(filename)\n",
    "        sentences = getSentencesFromTXT(filename)\n",
    "    corpus_sentences[website] = classifySentences(vectorizers, models, sentences)\n",
    "    \n",
    "print (\"{} files processed\\n\".format(len(corpus_sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out an example of how to access a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We collect such commercial transactional messages so that we can better understand the behavior of the senders of such messages, and better understand our customer behavior and improve our products, services, and advertising.\n"
     ]
    }
   ],
   "source": [
    "print (corpus_sentences['unroll.me']['First Party Collection/Use'][5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write corpus_sentences to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./Data/unrollme.pk', 'wb') as f:\n",
    "    pickle.dump(corpus_sentences, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
