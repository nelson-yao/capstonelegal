{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle \n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import data_helpers\n",
    "from text_cnn import TextCNN\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## read in the training data\n",
    "with open(\"../Data/sentence_score_conso10_py2.pk\", 'rb') as f:\n",
    "    data=cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Website', u'Sentence', u'Data Retention', u'Data Security',\n",
       "       u'Do Not Track', u'First Party Collection/Use',\n",
       "       u'International and Specific Audiences', u'Not_used', u'Policy Change',\n",
       "       u'Third Party Sharing/Collection', u'User Access, Edit and Deletion',\n",
       "       u'User Choice/Control'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Privacy', 'Policy', 'Last', 'Revised:', 'April,', '2011']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Sentence\"].values\n",
    "re.split(\"\\s+\", ' Privacy Policy    Last Revised: April, 2011'.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[[ 1.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "## testing the one-hot generation\n",
    "## assuming input is a discrete-level array\n",
    "test=np.array([1,2,2,3,4,2])\n",
    "nlevels=len(np.unique(test))\n",
    "correction=np.min(test)\n",
    "print(nlevels)\n",
    "testOneHot=np.zeros((len(test), nlevels))\n",
    "testOneHot[range(len(test)), [int(x)-correction for x in test]]=1\n",
    "print testOneHot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.,  0.]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testOneHot[1::,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Website', u'Sentence', u'Data Retention', u'Data Security',\n",
       "       u'Do Not Track', u'First Party Collection/Use',\n",
       "       u'International and Specific Audiences', u'Not_used', u'Policy Change',\n",
       "       u'Third Party Sharing/Collection', u'User Access, Edit and Deletion',\n",
       "       u'User Choice/Control'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cnn_helpers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cnn_helpers.py\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from tensorflow.contrib import learn\n",
    "from glove import GloVe\n",
    "\n",
    "def clean_str(string):\n",
    "    \n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    #string = re.sub(r\"e-mail\", \"email\", string)\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    string = re.sub(r\":\", \" \", string)\n",
    "    \n",
    "    return string.strip().lower()\n",
    "\n",
    "def padSentence(wordlist, maximumLength, pad=\"<PAD>\"):\n",
    "    paddedList=wordlist+[pad]*(maximumLength-len(wordlist))\n",
    "    return \" \".join(paddedList)\n",
    "\n",
    "\n",
    "def processWord(scoretable, category, trainproportion, seed=120, minscore=-2, maxscore=2):\n",
    "    \"\"\"Turns sentences to lists of word indices, padded to maximum sentence length in the corpus.\n",
    "    Use tf.contrib.learn.VocabProcessor to generate word indices and paddings\n",
    "    Process all the words first, then split the data into training and development\n",
    "    \"\"\"\n",
    "    \n",
    "    # Assuming the input label is a one-dimensional discrete-valued array\n",
    "    # trainportion means the percentage of data to be used as training, the rest are used as validation\n",
    "    # choose a category, get the data from the data file\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    ## if select all sentences at once, scores are averaged across categories \n",
    "    if category.lower()==\"all\":  \n",
    "        \n",
    "    else:\n",
    "        categoryFrame=scoretable[[\"Sentence\", category]]\n",
    "        \n",
    "    nonmissing=categoryFrame.dropna()\n",
    "    \n",
    "    ## equal portion sampling, sample from both positive and negative group\n",
    "    #shufflednonmissing=nonmissing.sample(frac=1, random_state=seed)   # shuffle the data \n",
    "    \n",
    "    nonmissingNumber=nonmissing.shape[0]\n",
    "\n",
    "    texts=nonmissing[\"Sentence\"].tolist()\n",
    "    max_length = max([len(x.split(\" \")) for x in texts])\n",
    "    cleantexts=[clean_str(sent) for sent in texts]\n",
    "    \n",
    "    processor = learn.preprocessing.VocabularyProcessor(max_length)\n",
    "    features=np.array(list(processor.fit_transform(cleantexts)))\n",
    "    \n",
    "    labels=nonmissing[category].tolist()\n",
    "    \n",
    "    nlevels=maxscore-minscore+1\n",
    "\n",
    "    correction=minscore\n",
    "    \n",
    "    labelsOneHot=np.zeros((len(labels), nlevels))\n",
    "    labelsOneHot[range(len(labels)), [int(x-correction) for x in labels]]=1\n",
    "    \n",
    "    train_index=int(nonmissingNumber*trainproportion)\n",
    "    random_indices=np.random.permutation(range(nonmissingNumber))\n",
    "    \n",
    "    shuffledFeatures=features[random_indices]\n",
    "    shuffledLabels=labelsOneHot[random_indices]\n",
    "    \n",
    "    trainFeatures=shuffledFeatures[0:train_index]\n",
    "    devFeatures=shuffledFeatures[train_index+1::]\n",
    "    \n",
    "    trainLabels=shuffledLabels[0:train_index]\n",
    "    devLabels=shuffledLabels[train_index+1::]\n",
    "    \n",
    "    assert trainFeatures.shape[0]==trainLabels.shape[0], \"Number of training features and labels don't match\"\n",
    "    assert devFeatures.shape[0]==devLabels.shape[0], \"Number of development features and labels don't match\"\n",
    "    vocabsize=len(processor.vocabulary_)\n",
    "    return trainFeatures, devFeatures, trainLabels, devLabels, processor\n",
    "    \n",
    "    \n",
    "def processGlove(scoretable, category, trainproportion, gloveFile, seed=120, minscore=-2, maxscore=2):\n",
    "    \"\"\"\n",
    "    generates training features and labels, and a GloVe embedding in the form of a numpy array\n",
    "    glove is a GloVe object, defined in glove.py\n",
    "    \"\"\"\n",
    "    \n",
    "    # Assuming the input label is a one-dimensional discrete-valued array\n",
    "    # trainportion means the percentage of data to be used as training, the rest are used as validation\n",
    "    # choose a category, get the data from the data file\n",
    "    np.random.seed(seed)\n",
    "    categoryFrame=scoretable[[\"Sentence\", category]]\n",
    "    nonmissing=categoryFrame.dropna()\n",
    "    \n",
    "    ## equal portion sampling, sample from both positive and negative group\n",
    "    #shufflednonmissing=nonmissing.sample(frac=1, random_state=seed)   # shuffle the data \n",
    "    \n",
    "    nonmissingNumber=nonmissing.shape[0]\n",
    "\n",
    "    texts=nonmissing[\"Sentence\"].tolist()\n",
    "    max_length = max([len(x.split(\" \")) for x in texts])\n",
    "    cleantexts=[clean_str(sent) for sent in texts]\n",
    "    \n",
    "    processor = learn.preprocessing.VocabularyProcessor(max_length)\n",
    "    features=np.array(list(processor.fit_transform(cleantexts)))\n",
    "    vocabsize=len(processor.vocabulary_)\n",
    "    \n",
    "    ## make an embedding matrix\n",
    "    glove=GloVe(gloveFile)\n",
    "    embeddings=np.zeros((vocabsize, glove.n_dim))\n",
    "    mappings=processor.vocabulary_._mapping\n",
    "    \n",
    "    for word, index in mappings.items():\n",
    "        embeddings[index]=glove[word]\n",
    "\n",
    "    ## Generate one-hot labels\n",
    "    labels=nonmissing[category].tolist()\n",
    "    nlevels=maxscore-minscore+1\n",
    "    correction=minscore\n",
    "    \n",
    "    labelsOneHot=np.zeros((len(labels), nlevels))\n",
    "    labelsOneHot[range(len(labels)), [int(x-correction) for x in labels]]=1\n",
    "    \n",
    "    train_index=int(nonmissingNumber*trainproportion)\n",
    "    random_indices=np.random.permutation(range(nonmissingNumber))\n",
    "    \n",
    "    shuffledFeatures=features[random_indices]\n",
    "    shuffledLabels=labelsOneHot[random_indices]\n",
    "    \n",
    "    trainFeatures=shuffledFeatures[0:train_index]\n",
    "    devFeatures=shuffledFeatures[train_index+1::]\n",
    "    \n",
    "    trainLabels=shuffledLabels[0:train_index]\n",
    "    devLabels=shuffledLabels[train_index+1::]\n",
    "    \n",
    "    assert trainFeatures.shape[0]==trainLabels.shape[0], \"Number of training features and labels don't match\"\n",
    "    assert devFeatures.shape[0]==devLabels.shape[0], \"Number of development features and labels don't match\"\n",
    "    \n",
    "    return trainFeatures, devFeatures, trainLabels, devLabels, embeddings, processor\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def batch_iter(train, label, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    data = np.array(list(zip(train, label)))\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object transform at 0x7f2529f38a50>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor = learn.preprocessing.VocabularyProcessor(150)\n",
    "processor.fit_transform(data[\"Sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<UNK>\n"
     ]
    }
   ],
   "source": [
    "mapping=processor.vocabulary_._mapping\n",
    "for key, value in mapping.items():\n",
    "    if value==0:\n",
    "        print key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from glove import GloVe\n",
    "import glove\n",
    "reload(glove)\n",
    "glove100=glove.GloVe(\"/home/nyao/Embeddings/glove.6B.100d.mini.txt\")\n",
    "\n",
    "sortedvocab=sorted(glove100.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cnn_helpers\n",
    "reload(cnn_helpers)\n",
    "train_x, dev_x, train_y, dev_y, embeddings, processor=cnn_helpers.processGlove(data, \"Data Security\", 0.85, \"/home/nyao/Embeddings/glove.6B.100d.mini.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2014"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processor.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processBow(scoretable, category, trainproportion, regionsize, seed=120, minscore=-2, maxscore=2):\n",
    "    \"\"\"\n",
    "    generates training features and labels, and a GloVe embedding in the form of a numpy array\n",
    "    glove is a GloVe object, defined in glove.py\n",
    "    \"\"\"\n",
    "    \n",
    "    # Assuming the input label is a one-dimensional discrete-valued array\n",
    "    # trainportion means the percentage of data to be used as training, the rest are used as validation\n",
    "    # choose a category, get the data from the data file\n",
    "    np.random.seed(seed)\n",
    "    categoryFrame=scoretable[[\"Sentence\", category]]\n",
    "    nonmissing=categoryFrame.dropna()\n",
    "    \n",
    "    ## equal portion sampling, sample from both positive and negative group\n",
    "    #shufflednonmissing=nonmissing.sample(frac=1, random_state=seed)   # shuffle the data \n",
    "    \n",
    "    nonmissingNumber=nonmissing.shape[0]\n",
    "    \n",
    "    texts=nonmissing[\"Sentence\"].tolist()\n",
    "    max_length = max([len(x.split(\" \")) for x in texts])\n",
    "    cleantexts=[clean_str(sent) for sent in texts]\n",
    "    \n",
    "    processor = learn.preprocessing.VocabularyProcessor(max_length)\n",
    "    features=np.array(list(processor.fit_transform(cleantexts)))\n",
    "    vocabsize=len(processor.vocabulary_)\n",
    "    \n",
    "    ## make an embedding matrix\n",
    "    glove=GloVe(gloveFile)\n",
    "    embeddings=np.zeros((vocabsize, glove.n_dim))\n",
    "    mappings=processor.vocabulary_._mapping\n",
    "    \n",
    "    for word, index in mappings.items():\n",
    "        embeddings[index]=glove[word]\n",
    "\n",
    "    ## Generate one-hot labels\n",
    "    labels=nonmissing[category].tolist()\n",
    "    nlevels=maxscore-minscore+1\n",
    "    correction=minscore\n",
    "    \n",
    "    labelsOneHot=np.zeros((len(labels), nlevels))\n",
    "    labelsOneHot[range(len(labels)), [int(x-correction) for x in labels]]=1\n",
    "    \n",
    "    train_index=int(nonmissingNumber*trainproportion)\n",
    "    random_indices=np.random.permutation(range(nonmissingNumber))\n",
    "    \n",
    "    shuffledFeatures=features[random_indices]\n",
    "    shuffledLabels=labelsOneHot[random_indices]\n",
    "    \n",
    "    # generate a BOW matrix:\n",
    "    \n",
    "    \n",
    "    trainFeatures=shuffledFeatures[0:train_index]\n",
    "    devFeatures=shuffledFeatures[train_index+1::]\n",
    "    \n",
    "    trainLabels=shuffledLabels[0:train_index]\n",
    "    devLabels=shuffledLabels[train_index+1::]\n",
    "    \n",
    "    assert trainFeatures.shape[0]==trainLabels.shape[0], \"Number of training features and labels don't match\"\n",
    "    assert devFeatures.shape[0]==devLabels.shape[0], \"Number of development features and labels don't match\"\n",
    "    \n",
    "    return trainFeatures, devFeatures, trainLabels, devLabels, embeddings, processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic CNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5)\n",
      "(1, 3, 5, 1)\n"
     ]
    }
   ],
   "source": [
    "test=np.array([[1,2,3,4,2],[2,3,4,22,1], [3,4,2,3,2]])\n",
    "testsess=tf.Session()\n",
    "X=tf.placeholder(tf.int32, [None, 5])\n",
    "y=tf.expand_dims(X, -1)\n",
    "print(test.shape)\n",
    "with testsess.as_default():\n",
    "    out=testsess.run([y], feed_dict={X:test})\n",
    "print(np.array(out).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cnn_net_glove.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cnn_net_glove.py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class CNN(object):\n",
    "    \"\"\"\n",
    "    convolutional neural net obejct for label classification\n",
    "    Use pre-trained GloVe embeddings\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size,\n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        #self.input_suprise = tf.placeholder(tf.float32, [None, 1], name=\"input_surprise\")\n",
    "\n",
    "        # L2 loss function\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        #with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            W = tf.Variable(\n",
    "                tf.constant(0.0, shape=[vocab_size, embedding_size]),\n",
    "                name=\"Embedding_Weights\", trainable=False)\n",
    "            self.embedding_placeholder=tf.placeholder(tf.float32, shape=[vocab_size, embedding_size])\n",
    "            self.embedding_init=W.assign(self.embedding_placeholder)\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "        \n",
    "        # Convolution + maxpool layer give each filter\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],        #strides\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "\n",
    "                # Relu\n",
    "                relu = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "\n",
    "                # Max pooling, works better\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    relu,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                \n",
    "                # Average pooling\n",
    "    \n",
    "#                 pooled = tf.nn.avg_pool(\n",
    "#                     relu,\n",
    "#                     ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "#                     strides=[1, 1, 1, 1],\n",
    "#                     padding='VALID',\n",
    "#                     name=\"pool\")\n",
    "        \n",
    "#                 pooled_outputs.append(pooled)\n",
    "\n",
    "\n",
    "        num_filters_total = num_filters * len(filter_sizes) \n",
    "        self.h_pool = tf.concat(pooled_outputs,3 )\n",
    "        #features = tf.concat(1, [self.input_suprise, self.h_pool])\n",
    "\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Initial prediction\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "\n",
    "            self.xw_out = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.xw_out, 1, name=\"predictions\")\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.xw_out, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cnn_glove_train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cnn_glove_train.py\n",
    "from __future__ import division\n",
    "import tensorflow as tf\n",
    "import cPickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import data_helpers\n",
    "from text_cnn import TextCNN\n",
    "from tensorflow.contrib import learn\n",
    "from sklearn.metrics import f1_score\n",
    "from cnn_net_glove import CNN\n",
    "import cnn_helpers\n",
    "from sklearn.metrics import average_precision_score\n",
    "import logging\n",
    "import json\n",
    "import sys\n",
    "\n",
    "timestamp = str(int(time.time()))\n",
    "\n",
    "parser=argparse.ArgumentParser(description=\"Arguments for CNN training\")\n",
    "parser.add_argument(\"InputData\", help=\"Path to the input pickle file\", action=\"store\")\n",
    "parser.add_argument(\"Category\", help=\"Name of the category to focus on\")\n",
    "parser.add_argument(\"embedding_File\", default=None, action=\"store\",help=\"Path to the embedding file\")\n",
    "\n",
    "parser.add_argument(\"--train-proportion\",default=0.85, type=float, dest=\"train_proportion\", help=\"Proportion of the data to be used as training set, the rest is used as development set, default is 0.85\")\n",
    "parser.add_argument(\"--filter-size\", type=int, default=[2,3,5],dest=\"filter_sizes\", nargs=\"+\",help=\"Sizes of the filters, can specify mutiple sizes for multiple channels. Default is [2,3,5]\")\n",
    "parser.add_argument(\"--model-name\", default=\"CNN\", dest=\"model_name\", help=\"Name the model, this will be the folder name under which everything is saved. Default is \\\"CNN\\\" followed by category code and timestamp\")\n",
    "parser.add_argument(\"--num-filters\", default=100, dest=\"num_filters\", help=\"Number of filters per channel, default 100\")\n",
    "parser.add_argument(\"--dropout-keep-prob\", default=0.5, dest=\"dropout_keep_prob\",type=float, help=\"Drop out keep probability, default 0.5\")\n",
    "parser.add_argument(\"--l2\", default=1.0, type=float, dest=\"l2\", help=\"L2 regularization parameter, default 1.0\")\n",
    "parser.add_argument(\"--batch-size\", default=100, type=int, dest=\"batch_size\", help=\"Batch size, default 100\")\n",
    "parser.add_argument(\"--num-epochs\", default=100, type=int, dest=\"num_epoch\", help=\"Number of epochs, default 100\")\n",
    "parser.add_argument(\"--print-every\", default=100, type=int, dest=\"print_every\", help=\"Print after how many steps, default is 100\")\n",
    "parser.add_argument(\"--save-every\", default=500, type=int, dest=\"save_every\", help=\"Save a checkpoint after every N steps, default is 500. A final checkpoint will also be saved after training is finished\")\n",
    "parser.add_argument(\"--eval-every\", default=500, type=int, dest=\"eval_every\", help=\"Evalute the model after every N steps, default is 500\")\n",
    "parser.add_argument(\"--descriptor\", default=\"CNN\", dest=\"descriptor\", help=\"A short descriptor for the model, will be used in the performace records\")\n",
    "\n",
    "\n",
    "\n",
    "args=parser.parse_args()\n",
    "\n",
    "InputData=args.InputData\n",
    "Category=args.Category  # category to focus on\n",
    "train_proportion=args.train_proportion\n",
    "\n",
    "categoryCode={'Data Retention':\"DR\", 'Data Security':\"DS\", \n",
    "              'Do Not Track':\"DNT\", 'First Party Collection/Use':\"FP\", 'International and Specific Audiences':\"INT\",\n",
    "              'Not_used':\"NU\", 'Policy Change':\"PC\", 'Third Party Sharing/Collection':\"TP\", \n",
    "              'User Access, Edit and Deletion':\"UA\",'User Choice/Control':\"UC\"}\n",
    "\n",
    "#CNN  paramters\n",
    "\n",
    "embeddingFile=args.embedding_File  ## path to the embedding file\n",
    "FS = args.filter_sizes #Filter Sizes\n",
    "Num_F = args.num_filters  #Number of filters per filter size\n",
    "dropout_keep_prob = args.dropout_keep_prob\n",
    "L = args.l2\n",
    "batch_size = args.batch_size\n",
    "num_epoch = args.num_epoch\n",
    "print_every=args.print_every\n",
    "save_every=args.save_every\n",
    "eval_every=args.eval_every\n",
    "\n",
    "# naming\n",
    "try:\n",
    "    catCode=categoryCode[Category]\n",
    "except KeyError:\n",
    "    print(\"Input category does not exist\")\n",
    "    sys.exit()\n",
    "#\n",
    "model_name=args.model_name+\"_\"+catCode+\"_\"+timestamp[-4::]\n",
    "\n",
    "descriptor=args.descriptor\n",
    "## define logging control\n",
    "\n",
    "if embeddingFile is None:\n",
    "    print(\"No embedding file found, please enter the path\")\n",
    "    embeddingFile=input(\"Enter full path to embedding file : \")\n",
    "    \n",
    "out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", model_name))\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "    \n",
    "logger = logging.getLogger(model_name)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "fh = logging.FileHandler(os.path.join(out_dir, model_name+\".log\"))\n",
    "fh.setLevel(logging.DEBUG)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "formatter1 = logging.Formatter('%(asctime)s %(levelname)s : %(message)s')\n",
    "formatter2 = logging.Formatter('%(asctime)s %(levelname)s : %(message)s')\n",
    "\n",
    "ch.setFormatter(formatter1)\n",
    "fh.setFormatter(formatter2)\n",
    "\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "logger.info(\"CNN classifier using GloVe embeddings, with AdamOptimizer\")\n",
    "logger.info(\"Reading data from {}\".format(InputData))\n",
    "logger.info(\"Training models to predict scores the category of \\\"{}\\\" \".format(Category))\n",
    "\n",
    "logger.info(\"Filter sizes : {}\".format(FS))\n",
    "logger.info(\"Number of filtes : {}\" .format(Num_F))\n",
    "logger.info(\"Dropout keep probability : {}\" .format(dropout_keep_prob))\n",
    "logger.info(\"L2 regularization constant : {}\" .format(L))\n",
    "logger.info(\"Batch size : {}\" .format(batch_size))\n",
    "logger.info(\"Number of epochs : {}\".format(num_epoch))\n",
    "\n",
    "\n",
    "with open(InputData, 'rb') as f:\n",
    "    data=cPickle.load(f)\n",
    "    \n",
    "## Generate training data\n",
    "logger.info(\"Preparing data and embeddings\")\n",
    "train_x, dev_x, train_y, dev_y, embeddings, processor=cnn_helpers.processGlove(data,Category , train_proportion, embeddingFile)\n",
    "vocabsize=len(processor.vocabulary_)\n",
    "M=embeddings.shape[1]   # embedding size\n",
    "\n",
    "logger.info(\"Embedding matrix shape : {}\".format(embeddings.shape))\n",
    "logger.info(\"Vocabulary size : {}\".format(vocabsize))\n",
    "logger.info(\"Training sample size : {}\".format(train_x.shape[0]))\n",
    "logger.info(\"Development sample size : {}\".format(dev_x.shape[0]))\n",
    "\n",
    "totalsteps=train_x.shape[0]*num_epoch/batch_size\n",
    "\n",
    "processorfile=os.path.join(out_dir, model_name+\"_vocab.pk\")\n",
    "logger.info(\"Saving vocabulary processor to {}\".format(processorfile))\n",
    "with open(\"processorfile\", \"wb\") as fv:\n",
    "    cPickle.dump(processor, fv)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "        cnn = CNN(\n",
    "            sequence_length=train_x.shape[1],\n",
    "            num_classes=train_y.shape[1],\n",
    "            vocab_size=vocabsize,\n",
    "            embedding_size = M,\n",
    "            filter_sizes = FS,\n",
    "            num_filters = Num_F,\n",
    "            l2_reg_lambda = L)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(1e-3) \n",
    "        \n",
    "        # AdamOptimizer achieves much better results than Adadelta\n",
    "        #optimizer = tf.train.AdadeltaOptimizer(0.001)\n",
    "        \n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "        \n",
    "        # Output directory for checkpoints and summaries\n",
    "        \n",
    "        logger.info(\"Writing to {}\\n\".format(out_dir))\n",
    "       \n",
    "        # Train Summaries\n",
    "\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        init_op=tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        # Initialize all variables\n",
    "        sess.run(init_op)\n",
    "        # feed the embedding matrix\n",
    "        sess.run(cnn.embedding_init, feed_dict={cnn.embedding_placeholder: embeddings})\n",
    "        \n",
    "        def train_step(x_batch, y_batch, printevery, logger=logger):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              \n",
    "              cnn.dropout_keep_prob: dropout_keep_prob\n",
    "            }\n",
    "            _, step,loss, accuracy = sess.run(\n",
    "                [train_op, global_step, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            if step % printevery == 0:\n",
    "                logger.info(\"step {} / {}, loss {:g}, acc {:g}\".format( step, int(totalsteps), loss, accuracy))\n",
    "\n",
    "\n",
    "        def dev_step(x_batch_dev, y_batch_dev, logger=logger):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "        \n",
    "            print(\"\\nDev Set Evaluation:\")\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch_dev,\n",
    "              cnn.input_y: y_batch_dev,\n",
    "              cnn.dropout_keep_prob: dropout_keep_prob\n",
    "            }\n",
    "            step, loss, scores,prediction = sess.run(\n",
    "                [global_step, cnn.loss,cnn.xw_out, cnn.predictions],\n",
    "                feed_dict)\n",
    "            \n",
    "            f1=f1_score(y_batch_dev.argmax(axis=1), prediction, average='micro')\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            logger.info(\"step {} / {}, loss {:g}, dev f1-score {}\".format(step,int(totalsteps), loss, f1))\n",
    "            return f1\n",
    "\n",
    "\n",
    "        # Generating batches\n",
    "        batches = cnn_helpers.batch_iter(\n",
    "            train_x, train_y, batch_size, num_epoch)\n",
    "        \n",
    "        # Training executions\n",
    "        devbatches=cnn_helpers.batch_iter(dev_x, dev_y, 100, 1)\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch, printevery=print_every)\n",
    "            \n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % eval_every == 0:\n",
    "                #log.write(\"\\nDev Set Evaluation:\\n\")\n",
    "                dev_f1=dev_step(dev_x, dev_y)\n",
    "                \n",
    "            if current_step % save_every==0:\n",
    "                \n",
    "                logger.info(\"Saving checkpoint to {}/model_{}.ckpt\".format(checkpoint_dir, current_step))\n",
    "                save_path = saver.save(sess, \"{}/model{}.ckpt\".format(checkpoint_dir, current_step))\n",
    "                \n",
    "                performance={}\n",
    "                performance[\"Embedding size\"]=M\n",
    "                performance[\"Filter number\"]=Num_F\n",
    "                performance[\"Filter sizes\"]=\",\".join([str(x) for x in FS])\n",
    "                performance[\"Dropout keep probability\"]=dropout_keep_prob\n",
    "                performance[\"Regularization\"]=L\n",
    "                performance[\"Batch size\"]=batch_size\n",
    "                performance[\"Development f1\"]=dev_f1\n",
    "                performance[\"Model Type\"]=descriptor\n",
    "                performance[\"Model Name\"]=model_name\n",
    "                \n",
    "                performancefile=os.path.join(out_dir, model_name+\"_performance.json\")\n",
    "                logger.info(\"Saving performances to {}\".format(performancefile))\n",
    "                with open(performancefile, \"w\") as fp:\n",
    "                    json.dump(performance, fp)\n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "        current_step = tf.train.global_step(sess, global_step)\n",
    "        dev_f1=dev_step(dev_x, dev_y)\n",
    "        logger.info(\"Saving checkpoint to {}/model_{}.ckpt\".format(checkpoint_dir, current_step))\n",
    "        save_path = saver.save(sess, \"{}/model_{}.ckpt\".format(checkpoint_dir, current_step))\n",
    "        \n",
    "        \n",
    "\n",
    "        with open(performancefile, \"w\") as fp:\n",
    "            json.dump(performance, fp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.sh\n",
    "# python cnn_glove_train.py \"../Data/sentence_score_conso10_py2.pk\" \"Data Security\" \\\n",
    "# \"/home/nyao/Embeddings/glove.6B.100d.txt\"  --descriptor \"CNN_glove\" --num-epochs 100 --print-every 50 --eval-every 100 \\\n",
    "# --model-name \"CNN_glove_Adam_\"\n",
    "\n",
    "python cnn_glove_train.py \"../Data/sentence_score_conso10_py2.pk\" 'User Choice/Control' \\\n",
    "\"/home/nyao/Embeddings/glove.6B.100d.txt\"  --descriptor \"CNN_glove\" --num-epochs 500 --print-every 200 \\\n",
    "--eval-every 600  --model-name \"CNN_glove\" --filter-size 3 4 5  --dropout-keep-prob 0.8 --save-every 600\n",
    "    \n",
    "python cnn_glove_train.py \"../Data/sentence_score_conso10_py2.pk\" 'User Choice/Control' \\\n",
    "\"/home/nyao/Embeddings/glove.6B.100d.txt\"  --descriptor \"CNN_glove\" --num-epochs 500 --print-every 200 \\\n",
    "--eval-every 600  --model-name \"CNN_glove\" --filter-size 2 3 4 5   --dropout-keep-prob 0.8 --save-every 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Website',\n",
       " u'Sentence',\n",
       " u'Data Retention',\n",
       " u'Data Security',\n",
       " u'Do Not Track',\n",
       " u'First Party Collection/Use',\n",
       " u'International and Specific Audiences',\n",
       " u'Not_used',\n",
       " u'Policy Change',\n",
       " u'Third Party Sharing/Collection',\n",
       " u'User Access, Edit and Deletion',\n",
       " u'User Choice/Control']"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  GloVe embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
