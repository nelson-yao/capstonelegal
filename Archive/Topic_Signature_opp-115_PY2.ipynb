{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd \n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup \n",
    "from IPython.display import display, HTML\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n"
     ]
    }
   ],
   "source": [
    "## Read in consolidated annotations\n",
    "#annotations={}\n",
    "#annofiles=glob.glob(\"Data/opp115-parsed-annotation-1.0/*.json\")\n",
    "#print len(annofiles)\n",
    "policyFiles=glob.glob(\"/share/pub/OPP-115/sanitized_policies/*.html\")\n",
    "print(len(policyFiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###  Read in the annotations and original policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def readAnno(filelist):\n",
    "  annotations={}\n",
    "  for filename in filelist:\n",
    "    website=re.sub(\".json\", '', os.path.basename(filename))\n",
    "    with open(filename, \"r\") as f:\n",
    "      annotations[website]=json.load(f)\n",
    "    f.close()\n",
    "  return annotations\n",
    "\n",
    "def readPolicies(filelist):\n",
    "  soups={}\n",
    "  for filename in filelist:\n",
    "    base=os.path.basename(filename).split(\"_\")[1]\n",
    "    website=re.sub(\".html\", '', base)\n",
    "    soups[website]=BeautifulSoup(open(filename, \"r\").read(), 'html.parser')\n",
    "  return soups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(\"Data/parsed-annotation-0.5.pk\", 'rb') as f:\n",
    "  annotations=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endIndexInSegment</th>\n",
       "      <th>section</th>\n",
       "      <th>selectedText</th>\n",
       "      <th>startIndexInSegment</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>153</td>\n",
       "      <td>18</td>\n",
       "      <td>promotional purpose through one of our websites</td>\n",
       "      <td>106</td>\n",
       "      <td>Marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201</td>\n",
       "      <td>18</td>\n",
       "      <td>or to make a purchase from the PlayStation Shop</td>\n",
       "      <td>154</td>\n",
       "      <td>Perform service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>226</td>\n",
       "      <td>22</td>\n",
       "      <td>Email addresses collected from consumers durin...</td>\n",
       "      <td>0</td>\n",
       "      <td>Perform service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200</td>\n",
       "      <td>24</td>\n",
       "      <td>so that we may assist these customers with cur...</td>\n",
       "      <td>124</td>\n",
       "      <td>Perform service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>159</td>\n",
       "      <td>40</td>\n",
       "      <td>necessary to fulfill the purposes outlined in ...</td>\n",
       "      <td>102</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   endIndexInSegment  section  \\\n",
       "0                153       18   \n",
       "1                201       18   \n",
       "2                226       22   \n",
       "3                200       24   \n",
       "4                159       40   \n",
       "\n",
       "                                        selectedText  startIndexInSegment  \\\n",
       "0    promotional purpose through one of our websites                  106   \n",
       "1    or to make a purchase from the PlayStation Shop                  154   \n",
       "2  Email addresses collected from consumers durin...                    0   \n",
       "3  so that we may assist these customers with cur...                  124   \n",
       "4  necessary to fulfill the purposes outlined in ...                  102   \n",
       "\n",
       "             value  \n",
       "0        Marketing  \n",
       "1  Perform service  \n",
       "2  Perform service  \n",
       "3  Perform service  \n",
       "4            Other  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations[\"playstation.com\"][\"Data Retention\"]['Retention Purpose']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### read in sanitized policy texts\n",
    "policySoups=readPolicies(policyFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n",
      "115\n"
     ]
    }
   ],
   "source": [
    "## make sure that the website names are the same in both dictinoaries\n",
    "test1=filter(lambda website: website in annotations.keys(), policySoups.keys())\n",
    "test2=filter(lambda website: website in policySoups.keys(), annotations.keys())\n",
    "print(len(list(test1)))\n",
    "print(len(list(test2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Get a list of tokens from all of the policies, with frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Get the texts from beautifulsoup objects\n",
    "\n",
    "def extractTexts(soups):\n",
    "  policyTexts={}\n",
    "  for website in soups:\n",
    "    policyTexts[website]=soups[website].get_text()\n",
    "  return policyTexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## get texts from soup objects\n",
    "policyTexts=extractTexts(policySoups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## get the training and validation set \n",
    "import random\n",
    "websites=policyTexts.keys()\n",
    "seed=124\n",
    "random.seed(seed)\n",
    "trainWebsites=random.sample(websites, 105)\n",
    "valWebstes=[web for web in websites if web not in trainWebsites]\n",
    "\n",
    "trainTexts={web:policyTexts[web] for web in trainWebsites}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "import itertools\n",
    "import string\n",
    "engstop=set(stopwords.words(\"english\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print (string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "\n",
    "def cleantext(rawtext, remove):\n",
    "  # remove punctuations\n",
    "  cleaned3=remove.sub(\"\", rawtext.lower())\n",
    "  return cleaned3\n",
    "\n",
    "\n",
    "def getTokens(rawtext, length, stopwords):  # length defines number of words in the token, i.e. unigrams, bigrams ec\n",
    "  punctuations = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "  if length==1:\n",
    "    cleaned=cleantext(rawtext, punctuations)\n",
    "    tokenlist=word_tokenize(cleaned)\n",
    "    tokensNoStop=[token for token in tokenlist if token not in stopwords]\n",
    "    return list(set(tokensNoStop))\n",
    "  \n",
    "  if length>=2:\n",
    "    sentenceList=sent_tokenize(rawtext)\n",
    "    sentenceClean=[cleantext(sentence, punctuations) for sentence in sentenceList]\n",
    "    unigramLists=[word_tokenize(sentence) for sentence in sentenceClean]\n",
    "    \n",
    "    bigramLists=[zip(*[sentUnigram[i::] for i in range(length)]) for sentUnigram in unigramLists]\n",
    "    bigrams=list(itertools.chain.from_iterable(bigramLists))\n",
    "    \n",
    "    return list(set(bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getAllTokens(textDict, ngram, stopwords):\n",
    "  allTokens=[]\n",
    "  for website in textDict:\n",
    "    allTokens.extend(getTokens(textDict[website], ngram, stopwords))\n",
    "  return allTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Generate Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.79 s, sys: 88 ms, total: 7.88 s\n",
      "Wall time: 7.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainUnigrams=getAllTokens(trainTexts, 1, engstop)\n",
    "trainBigrams=getAllTokens(trainTexts, 2, engstop)\n",
    "trainTrigrams=getAllTokens(trainTexts, 3, engstop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49721\n",
      "154100\n",
      "188901\n"
     ]
    }
   ],
   "source": [
    "## Number of UNIQUE n-grams\n",
    "print(len(trainUnigrams))\n",
    "print(len(trainBigrams))\n",
    "print(len(trainTrigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Get annotation of the training documents\n",
    "trainAnno={key:annotations[key] for key in trainTexts.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\" name, email address, and your friend's email address. This information is collected and used only in a manner\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hondaSoup=policySoups[\"honda.com\"]\n",
    "hondaText=hondaSoup.get_text()\n",
    "hondaText.split(\"|||\")[52][250:360]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "psSoup=policySoups['playstation.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'endIndexInSegment', u'section', u'selectedText',\n",
       "       u'startIndexInSegment', u'value'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations['playstation.com']['First Party Collection/Use'][\"Collection Mode\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344\n"
     ]
    }
   ],
   "source": [
    "#print(annotations['playstation.com']['First Party Collection/Use'][\"Personal Information Type\"].keys())\n",
    "#print annotations['playstation.com']['First Party Collection/Use'][\"Personal Information Type\"][\"selectedText\"]\n",
    "\n",
    "print (annotations['playstation.com']['First Party Collection/Use'][\"Personal Information Type\"][\"startIndexInSegment\"][0])\n",
    "#print annotations['playstation.com']['First Party Collection/Use'][\"Personal Information Type\"]['endIndexInSegment'][0]\n",
    "\n",
    "textinfo=pd.DataFrame({\"selectedText\":annotations['playstation.com']['First Party Collection/Use'][\"Personal Information Type\"][\"selectedText\"],\\\n",
    "                      \"startIndexInSegment\": annotations['playstation.com']['First Party Collection/Use'][\"Personal Information Type\"][\"startIndexInSegment\"],\\\n",
    "                      \"endIndexInSegment\": annotations['playstation.com']['First Party Collection/Use'][\"Personal Information Type\"]['endIndexInSegment'],\\\n",
    "                      \"value\": annotations['playstation.com']['First Party Collection/Use'][\"Personal Information Type\"]['value']})\n",
    "\n",
    "\n",
    "textinfo=textinfo[[\"selectedText\", \"value\", \"startIndexInSegment\", \"endIndexInSegment\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "psSections=psSoup.get_text().split(\"|||\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'ionNetwork   In parts of North America and South America, Sony Network Entertainment America Inc. (\"SNEA\") operates Sony Online Services, a network of online games, movies, music, other media and content and communication services. PlayStation Network (\"PSN\") is one of these Sony Online Services. With a Sony Online Services or Sony Entertainment Network account, users can purchase goods and services from SNEA through Sony Online Services and may have the opportunity to participate in various network community activities. Users can register for and log into a Sony Entertainment Network account via us.playstation.com. Collection and '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psSections[6][41:680]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Determine Relevance of each sentence in training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1. Get the index of each sentences in the policy texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Label the sentences in a policy as relevant or irrelevant\n",
    "## \n",
    "# Topic: \"Personal Information Type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getSentIdx(raw):\n",
    "  allIdx={}\n",
    "  secList=raw.split(\"|||\")\n",
    "  for i in range(len(secList)):\n",
    "    try:\n",
    "      secText=secList[i]\n",
    "    except IndexError:\n",
    "      print(i)\n",
    "    secSents=sent_tokenize(secText)\n",
    "    secIdxLists=[]\n",
    "    for sent in secSents:\n",
    "      m=re.search(re.escape(sent), secText)  ## escape to account for quotes in the string\n",
    "      secIdxLists.append((sent, i, m.start(),m.end()))\n",
    "    allIdx[i]=secIdxLists\n",
    "  return allIdx\n",
    "\n",
    "def getAllIdx(textDict):\n",
    "  results={}\n",
    "  for website, text in textDict.items():\n",
    "    try:\n",
    "      senidxes=getSentIdx(text)\n",
    "      results[website]=senidxes\n",
    "    except IndexError:\n",
    "      print(website)\n",
    "  return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.25 s, sys: 8 ms, total: 8.26 s\n",
      "Wall time: 8.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainSentIdx=getAllIdx(trainTexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "endIndexInSegment                          912\n",
       "section                                      8\n",
       "selectedText           credit card information\n",
       "startIndexInSegment                        889\n",
       "value                                Financial\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### test to see if the indices are correct\n",
    "trainAnno[\"playstation.com\"]['First Party Collection/Use']['Personal Information Type'].loc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'Collection of personal information required to access certain website services may include the collection of date of birth, name, mailing address, email address or credit card information.',\n",
       " 8,\n",
       " 709,\n",
       " 897)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSentIdx[\"playstation.com\"][8][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164\n",
      "187\n",
      "('start index in text', 873)\n",
      "('end index in text', 896)\n",
      "('Length of sentence in Annotation, and raw text', 23, 23)\n"
     ]
    }
   ],
   "source": [
    "print( re.search(\"credit card information\", trainSentIdx[\"playstation.com\"][8][3][0]).start())\n",
    "print (re.search(\"credit card information\", trainSentIdx[\"playstation.com\"][8][3][0]).end())\n",
    "\n",
    "print (\"start index in text\", 709+164)\n",
    "print (\"end index in text\", 709+187)\n",
    "\n",
    "print (\"Length of sentence in Annotation, and raw text\", 912-889, 896-873)\n",
    "\n",
    "## for playstation.com anno indices are 16 characters ahead of copurs indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'WHAT WE COLLECT:   Collection of Personal Information through our Websites   We do not require that website visitors reveal any personally identifying information in order to gain general access to our websites.',\n",
       "  8,\n",
       "  0,\n",
       "  211),\n",
       " (u'However, visitors who do not wish to, or are not allowed by law to share personally identifying information, may not be able to access certain areas of our websites, participate in certain activities, or make a purchase from the PlayStationShop.',\n",
       "  8,\n",
       "  212,\n",
       "  457),\n",
       " (u'Although personally identifying information may be required to participate in certain promotions or features offered through our websites or to make a purchase from the PlayStationShop, participants provide this information on a voluntary basis only.',\n",
       "  8,\n",
       "  458,\n",
       "  708),\n",
       " (u'Collection of personal information required to access certain website services may include the collection of date of birth, name, mailing address, email address or credit card information.',\n",
       "  8,\n",
       "  709,\n",
       "  897)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainSentIdx[\"playstation.com\"][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Another example:\n",
    "exampleSite=\"honda.com\"\n",
    "annoEg=trainAnno[exampleSite]['First Party Collection/Use']['Personal Information Type'].loc[6]\n",
    "print (annoEg)\n",
    "selecTextAnno=annoEg[\"selectedText\"]\n",
    "\n",
    "print (selecTextAnno)\n",
    "\n",
    "print (\"\\n\")\n",
    "corpusText=trainSentIdx[\"honda.com\"][annoEg['section']]\n",
    "print (\"Text in corpus : \")\n",
    "display(corpusText)\n",
    "\n",
    "print (\"\\n\")\n",
    "sentenceNumber=0\n",
    "corpusStart=re.search(selecTextAnno, corpusText[sentenceNumber][0]).start()\n",
    "corpusEnd= re.search(selecTextAnno, corpusText[sentenceNumber][0]).end()\n",
    "\n",
    "print (\"start index in corpus section :\", corpusText[sentenceNumber][2]+corpusStart)\n",
    "print (\"end index in corpus section :\", corpusText[sentenceNumber][2]+corpusEnd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "##a third example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Another example:\n",
    "exampleSite=\"uptodate.com\"\n",
    "annoEg=trainAnno[exampleSite]['First Party Collection/Use']['Personal Information Type'].loc[4]\n",
    "print (annoEg)\n",
    "selecTextAnno=annoEg[\"selectedText\"]\n",
    "\n",
    "print (selecTextAnno)\n",
    "\n",
    "print (\"\\n\")\n",
    "corpusText=trainSentIdx[exampleSite][annoEg['section']]\n",
    "sentenceNumber=[idx for idx in range(len(corpusText))][0]\n",
    "\n",
    "print (\"Text in corpus : \")\n",
    "display(corpusText)\n",
    "\n",
    "print (\"\\n\")\n",
    "\n",
    "corpusStart=re.search(selecTextAnno, corpusText[sentenceNumber][0]).start()\n",
    "corpusEnd= re.search(selecTextAnno, corpusText[sentenceNumber][0]).end()\n",
    "\n",
    "print (\"start index in corpus section :\", corpusText[sentenceNumber][2]+corpusStart)\n",
    "print (\"end index in corpus section :\", corpusText[sentenceNumber][2]+corpusEnd)\n",
    "\n",
    "print (\"Annotation indices is ahead of corpus indices by {} characters \".format(annoEg[\"startIndexInSegment\"]-corpusStart))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainAnno[exampleSite]['First Party Collection/Use']['Personal Information Type'].loc[4][\"startIndexInSegment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather the sentences and put them in a convenient structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.6 s, sys: 208 ms, total: 48.8 s\n",
      "Wall time: 48.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# for each selectedText in annotation, search for corresponding sentence in the corpus \n",
    "# if a selectedText is more than one sentence long, it will be discarded, since it does not provide much information\n",
    "# for the importance of words\n",
    "def labelRel(annotations, sentIdx):\n",
    "  sentlabels={}\n",
    "  for website, siteanno in annotations.items():\n",
    "    sentlabels[website]={}\n",
    "    siteSents=sentIdx[website]\n",
    "    for section, sentList in siteSents.items():\n",
    "      \n",
    "      sentlabels[website][section]=[list(sentTuple) for sentTuple in sentList]\n",
    "      for sentEntry in sentlabels[website][section]:\n",
    "        sentEntry.append([])\n",
    "\n",
    "    for category in siteanno:\n",
    "      for topic in siteanno[category]:\n",
    "        topicFrame=siteanno[category][topic]\n",
    "        for idx in topicFrame.index:\n",
    "          if topicFrame.loc[idx][\"startIndexInSegment\"]!=-1 and topicFrame.loc[idx][\"value\"]!=\"Unspecified\":\n",
    "            entry=topicFrame.loc[idx]\n",
    "            anno_start=entry[\"endIndexInSegment\"]\n",
    "            anno_end=entry[\"startIndexInSegment\"]\n",
    "            corpusSents=sentlabels[website][entry[\"section\"]]\n",
    "            for sent in corpusSents:\n",
    "              corpus_start=sent[2]\n",
    "              corpus_end=sent[3]\n",
    "              if corpus_start <=anno_start and corpus_end >= anno_end:\n",
    "                sent[4].append((category, topic, entry[\"value\"]))\n",
    "              elif  abs(corpus_start-anno_start) <20 and abs(corpus_end-anno_end)<20:\n",
    "                sent[4].append((category,topic, entry[\"value\"]))\n",
    "              elif anno_start<=corpus_start and anno_end >= corpus_end:\n",
    "                sent[4].append((category,topic, entry[\"value\"]))\n",
    "                \n",
    "                \n",
    "  return sentlabels\n",
    "\n",
    "labeledTrainSents=labelRel(trainAnno, trainSentIdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'As part of the privacy program, we are subject to frequent audits of our sites and other enforcement and accountability mechanisms administered independently by ESRB.', 2, 467, 633, [('Data Security', 'Security Measure', 'Privacy/Security program'), ('Other', 'Other Type', 'Practice not covered')]]\n",
      "[ 'To protect your privacy to the maximum extent possible, we have undertaken this privacy initiative and our websites have been reviewed and certified by ESRB Privacy Online to meet established online information collection and use practices. As part of the privacy program, we are subject to frequent audits of our sites and other enforcement and accountability mechanisms administered independently by ESRB.']\n"
     ]
    }
   ],
   "source": [
    "print(labeledTrainSents[\"playstation.com\"][2][2])\n",
    "egAnno=trainAnno[\"playstation.com\"]['Data Security']['Security Measure']\n",
    "print(egAnno.loc[egAnno[\"section\"]==2, \"selectedText\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Collect all the sentences , along with their topics, for ease of processing later\n",
    "### Leave out the value for now\n",
    "\n",
    "def gatherSentsTopics(labeldIdxSet):\n",
    "  topiclist=[]\n",
    "  allsentences=[]\n",
    "  for website, corpus in labeldIdxSet.items():\n",
    "    for section, sentLists in corpus.items():\n",
    "      for sent in sentLists:\n",
    "        allsentences.append([sent[0], [(item[0], item[1]) for item in sent[4] if item[0]!=\"Other\"]])\n",
    "        for item in sent[4]:\n",
    "          topiclist.append((item[0], item[1]))\n",
    "  return set(topiclist), allsentences\n",
    "\n",
    "alltopics, allSentences=gatherSentsTopics(labeledTrainSents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'For example, Bing uses a cookie with a unique identifier known as the Search ID to operate the service and enable certain search features.',\n",
       " [('First Party Collection/Use', 'Collection Mode'),\n",
       "  ('First Party Collection/Use', 'Identifiability'),\n",
       "  ('First Party Collection/Use', 'Personal Information Type'),\n",
       "  ('First Party Collection/Use', 'Personal Information Type'),\n",
       "  ('First Party Collection/Use', 'Personal Information Type'),\n",
       "  ('First Party Collection/Use', 'Purpose'),\n",
       "  ('First Party Collection/Use', 'Purpose')]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allSentences[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## for each topic, we get alist of related sentences and a list of unrelated sentences\n",
    "\n",
    "def getTopicRelevanceList(labeledSentences, topiclist):\n",
    "  topicSentsCollection={topic:{\"Related\":[], \"Unrelated\":[]} for topic in topiclist}\n",
    "  for entry in labeledSentences:\n",
    "    labelset=set(entry[1])\n",
    "    for topic in topiclist:\n",
    "      if topic not in labelset:\n",
    "        topicSentsCollection[topic][\"Unrelated\"].append(entry[0])\n",
    "      else:\n",
    "         topicSentsCollection[topic][\"Related\"].append(entry[0])\n",
    "          \n",
    "  results={}\n",
    "  for topic in topiclist:\n",
    "    results[topic]={}\n",
    "    results[topic][\"Related\"]=list(set(topicSentsCollection[topic][\"Related\"]))\n",
    "    results[topic][\"Unrelated\"]=list(set(topicSentsCollection[topic][\"Unrelated\"]))\n",
    "  \n",
    "  return results\n",
    "\n",
    "\n",
    "relevantSetences=getTopicRelevanceList(allSentences, alltopics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1424\n",
      "1424\n"
     ]
    }
   ],
   "source": [
    "print(len(relevantSetences[('First Party Collection/Use', 'Does/Does Not')][\"Related\"]))\n",
    "print(len(set(relevantSetences[('First Party Collection/Use', 'Does/Does Not')][\"Related\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'For more information about Flash cookies and how to remove them from your computer, please see the paragraph below entitled \"SPECIAL NOTE - Flash Cookies.\"'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevantSetences[('User Choice/Control', 'Purpose')][\"Related\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate -2log(lambda) scores , version 1\n",
    "# n is the length of ngram, \n",
    "# get O11, O12, O21, and O22 counts, see Lin and Hovy paper\n",
    "from scipy.stats import binom\n",
    "def getCounts(relevantSents, ngramList, n):\n",
    "  #break each sentence to the type of ngrams\n",
    "  ngramCounts={}\n",
    "  topiclist=list(relevantSents.keys())\n",
    "  sentNgram={topic:{\"Related\":[], \"Unrelated\":[]} for topic in topiclist}\n",
    "  for topic, sentMap in relevantSents.items():\n",
    "    print(\"Setting Default for topic {}\".format(str(topic)))\n",
    "    ngramCounts.setdefault(topic, {})\n",
    "\n",
    "    for relevance, sentlist in sentMap.items():\n",
    "      for sent in sentlist:\n",
    "        sentNgram[topic][relevance].append(getTokens(sent, length=n,  stopwords=engstop))\n",
    "    #for ngram in ngramList:\n",
    "      #ngramCounts[topic].setdefault(ngram, Counter({\"O11\":0, \"O12\":0, \"O21\":0, \"O22\":0}))\n",
    "    \n",
    "  for topic, releNgrams in sentNgram.items():\n",
    "    print(\"Calculating Signature Scores for topic {}\".format(str(topic)))\n",
    "    for relSent in releNgrams[\"Related\"]:\n",
    "      for ngram in ngramList:\n",
    "        ngramCounts[topic].setdefault(ngram, Counter({\"O11\":0, \"O12\":0, \"O21\":0, \"O22\":0}))\n",
    "        if ngram in relSent:\n",
    "          ngramCounts[topic][ngram]+=Counter({\"O11\":1})\n",
    "        else:\n",
    "          ngramCounts[topic][ngram]+=Counter({\"O21\":1})\n",
    "          \n",
    "    for unrelSent in releNgrams[\"Unrelated\"]:\n",
    "      for ngram in ngramList:\n",
    "        ngramCounts[topic].setdefault(ngram, Counter({\"O11\":0, \"O12\":0, \"O21\":0, \"O22\":0}))\n",
    "        if ngram in unrelSent:\n",
    "          ngramCounts[topic][ngram]+=Counter({\"O12\":1})\n",
    "        else:\n",
    "          ngramCounts[topic][ngram]+=Counter({\"O22\":1})\n",
    "  \n",
    "  return ngramCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Get the scores for Unigrams, Bigrams and Trigrams\n",
    "unigramCounts=getCounts(relevantSetences, trainUnigrams, 1)\n",
    "#bigramCounts=getCounts(relevantSetences, trainBigrams, 2)\n",
    "#bigramCounts=getCounts(relevantSetences, trainTrigrams, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate -2log(lambda) scores, version 2\n",
    "# n is the length of ngram, \n",
    "# get O11, O12, O21, and O22 counts, see Lin and Hovy paper\n",
    "from scipy.stats import binom\n",
    "def getCountsTwo(relevantSents, ngramList, n):\n",
    "  #break each sentence to the type of ngrams\n",
    "  ngramCounts={}\n",
    "  topiclist=list(relevantSents.keys())\n",
    "  sentNgram={topic:{\"Related\":[], \"Unrelated\":[]} for topic in topiclist}\n",
    "  \n",
    "  for topic, sentMap in relevantSents.items():\n",
    "    print(\"Setting Default for topic {}\".format(str(topic)))\n",
    "    ngramCounts.setdefault(topic, {})\n",
    "    for relevance, sentlist in sentMap.items():\n",
    "      for sent in sentlist:\n",
    "        sentNgram[topic][relevance].append(getTokens(sent, length=n,  stopwords=engstop))\n",
    "        \n",
    "    for ngram in ngramList:\n",
    "      ngramCounts[topic].setdefault(ngram, Counter({\"O11\":0, \"O12\":0, \"O21\":0, \"O22\":0}))\n",
    "  \n",
    "  for topic, releNgrams in sentNgram.items():\n",
    "    print(\"Calculating Signature Scores for topic {}\".format(str(topic)))\n",
    "    for relSent in releNgrams[\"Related\"]:\n",
    "      counted=[]\n",
    "      for ngram in relSent:\n",
    "        ngramCounts[topic][ngram]+=Counter({\"O11\":1})\n",
    "        counted.append(ngram)\n",
    "        \n",
    "      left=[x for x in ngramList if x not in counted]\n",
    "      \n",
    "      for otherngram in left:\n",
    "        ngramCounts[topic][otherngram]+=Counter({\"O21\":1})\n",
    "          \n",
    "    for unrelSent in releNgrams[\"Unrelated\"]:\n",
    "      unrelCounted=[]\n",
    "      for ngram in unrelSent:\n",
    "        ngramCounts[topic][ngram]+=Counter({\"O12\":1})\n",
    "        unrelCounted.append(ngram)\n",
    "      unrelLeft=[x for x in ngramList if x not in unrelCounted]\n",
    "      \n",
    "      for otherngram in unrelLeft:\n",
    "        ngramCounts[topic][otherngram]+=Counter({\"O22\":1})\n",
    "  \n",
    "  return ngramCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "unigramCounts=getCountsTwo(relevantSetences, trainUnigrams, 1)\n",
    "#bigramCounts=getCounts(relevantSetences, trainBigrams, 2)\n",
    "#bigramCounts=getCounts(relevantSetences, trainTrigrams, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=set(range(20))\n",
    "b=a.difference(set(range(10)))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate -2log(lambda) scores , version 3\n",
    "# n is the length of ngram, \n",
    "# get O11, O12, O21, and O22 counts, see Lin and Hovy paper\n",
    "from scipy.stats import binom\n",
    "def getCountsV3(relevantSents, ngramList, n):\n",
    "  #break each sentence to the type of ngrams\n",
    "  ngramCounts={}\n",
    "  topiclist=list(relevantSents.keys())\n",
    "  ngramSet=set(ngramList)\n",
    "  sentNgram={topic:{\"Related\":[], \"Unrelated\":[]} for topic in topiclist}\n",
    "  \n",
    "  for topic, sentMap in relevantSents.items():\n",
    "    #print(\"Setting corpus Default for topic {}\".format(str(topic)))\n",
    "    ngramCounts.setdefault(topic, {})\n",
    "    for relevance, sentlist in sentMap.items():\n",
    "      for sent in sentlist:\n",
    "        sentNgram[topic][relevance].append(getTokens(sent, length=n,  stopwords=engstop))\n",
    "        \n",
    "    #print(\"Setting count Default for topic {}\".format(str(topic)))\n",
    "    for ngram in ngramList:\n",
    "      ngramCounts[topic].setdefault(\"O11\", Counter())\n",
    "      ngramCounts[topic].setdefault(\"O12\", Counter())\n",
    "      ngramCounts[topic].setdefault(\"O21\", Counter())\n",
    "      ngramCounts[topic].setdefault(\"O22\", Counter())\n",
    "    \n",
    "    \n",
    "  for topic, releNgrams in sentNgram.items():\n",
    "    #print(\"Calculating Signature Scores for topic {}\".format(str(topic)))\n",
    "    for relSent in releNgrams[\"Related\"]:\n",
    "      ngramCounts[topic][\"O11\"]+=Counter({ngram:1 for ngram in relSent})\n",
    "      ngramleft=ngramSet.difference(set(relSent))\n",
    "      ngramCounts[topic][\"O21\"]+=Counter({ngram:1 for ngram in ngramleft})\n",
    "      \n",
    "    for unrelSent in releNgrams[\"Unrelated\"]:\n",
    "      ngramCounts[topic][\"O12\"]+=Counter({ngram:1 for ngram in unrelSent})\n",
    "      ngramleftUnrel=ngramSet.difference(set(unrelSent))\n",
    "      ngramCounts[topic][\"O22\"]+=Counter({ngram:1 for ngram in ngramleftUnrel}) \n",
    "  \n",
    "  return ngramCounts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## calculate the counts for unigrams\n",
    "#unigramCounts=getCountsV3(relevantSetences, trainUnigrams, 1)\n",
    "# Time : 42 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Taking too long, Use Spark to calculate the counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Lambda Score\").master(\"spark://server1.capstonemly.com:7077\").getOrCreate()\n",
    "sc  = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../Results/relevantSetences.pk\",'rb') as f:\n",
    "  relevantSetences=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use the object relevantSentences\n",
    "# Define a function to put the topic and relevance with the sentences\n",
    "# Output: list of sublist with each sublist being [single topic, sentence, relevance ]\n",
    "def embedLabels(relSent):\n",
    "  results=[]\n",
    "  for topic, releSents in relSent.items():\n",
    "    for relevance, labeledSents in releSents.items():\n",
    "      for sentence in labeledSents:\n",
    "        results.append((topic, sentence, relevance))\n",
    "  return results\n",
    "\n",
    "relSentTuples=embedLabels(relevantSetences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('Other', 'Other Type'),\n",
       " u' How Does Disinformation Use My Information?',\n",
       " 'Unrelated')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relSentTuplesRDD=sc.parallelize(relSentTuples)\n",
    "relSentTuplesRDD.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Get bigram counts and scores\n",
    "#define a function to release from each sentence a stream of ngram with topics and whether the token is relevant or not \n",
    "# Each item in the stream is (topic, token, Observation{O11, O12, O21, O22}, 1)\n",
    "# From each sentence that are related, release the ngrams they have with the label \"O11\". \n",
    "# Also release all the other ngrams with the label \"O21\"\n",
    "\n",
    "# From each sentence that are unrelated to the topic, release the ngram they have with the label \"O12\"\n",
    "# Also release all the other ngrams not in the sentence with the label \"O22\"\n",
    "\n",
    "# Define a funciton applied to each element of the relSentTuples\n",
    "trainUnigramSet=set(trainUnigrams)\n",
    "trainBigramSet=set(trainBigrams)\n",
    "trainTrigramSet=set(trainTrigrams)\n",
    "\n",
    "\n",
    "def streamNgrams(sentTuple, n, ngramSet):\n",
    "  # define a label lookup mechanism to avoid if/else statements\n",
    "  assignlabel={\"In\":{\"Related\":\"O11\", \"Unrelated\":\"O12\"},\"Out\":{\"Related\":\"O21\", \"Unrelated\":\"O22\"}}\n",
    "  results=[]\n",
    "  relevance=sentTuple[2]\n",
    "  topic=sentTuple[0]\n",
    "  ngramList=getTokens(sentTuple[1], length=n, stopwords=engstop)\n",
    "  for ngram in ngramList:\n",
    "    label=assignlabel[\"In\"][relevance]\n",
    "    results.append(((topic, ngram, label), 1))\n",
    "    \n",
    "  ngramsLeft=ngramSet.difference(set(ngramList))\n",
    "  \n",
    "  for otherngram in ngramsLeft:\n",
    "    otherlabel=assignlabel[\"Out\"][relevance]\n",
    "    results.append(((topic, otherngram, otherlabel), 1))\n",
    "      \n",
    "  return results\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do a small test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9282"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test using one topic\n",
    "relSentTuplesTest=[x for x in relSentTuples if x[0]==('Third Party Sharing/Collection', 'Does/Does Not')]\n",
    "len(relSentTuplesTest)\n",
    "\n",
    "relSentTuplesTestRDD=sc.parallelize(relSentTuplesTest)\n",
    "testStream=relSentTuplesTestRDD.flatMap(lambda sentTuple: streamNgrams(sentTuple, 2, trainBigramSet))\n",
    "testCounts=testStream.reduceByKey(lambda a, b:a+b)\n",
    "testresult=testCounts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## count for a;; bigrams\n",
    "bigramStream=relSentTuplesRDD.flatMap(lambda sentTuple: streamNgrams(sentTuple, 2, trainBigramSet)).partitionBy(8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For each (Topic, ngram, Counts), sum up the result\n",
    "from operator import add\n",
    "bigramCounts=bigramStream.reduceByKey(lambda a, b:a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job 2 cancelled \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1375)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1622)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-cb88499babe1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbigramCounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/nyao/spark-2.1.0-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mfirst\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1359\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \"\"\"\n\u001b[0;32m-> 1361\u001b[0;31m         \u001b[0mrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nyao/spark-2.1.0-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1343\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1345\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nyao/spark-2.1.0-bin-hadoop2.6/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m    963\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m         \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nyao/spark-2.1.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nyao/spark-2.1.0-bin-hadoop2.6/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nyao/spark-2.1.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job 2 cancelled \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1375)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1622)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "bigramCounts.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job 13 cancelled \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1375)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1622)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:934)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-ea060e68269d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbigramCountsResult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbigramCounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/nyao/spark-2.1.0-bin-hadoop2.6/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \"\"\"\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nyao/spark-2.1.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nyao/spark-2.1.0-bin-hadoop2.6/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nyao/spark-2.1.0-bin-hadoop2.6/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job 13 cancelled \n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1375)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1622)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:934)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n"
     ]
    }
   ],
   "source": [
    "bigramCountsResult=bigramCounts.collect()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
