{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import gzip # for compressing Pickeled data\n",
    "import xml.etree.ElementTree as ET\n",
    "from textblob import TextBlob\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read vectorizers and classifiers from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with gzip.open('./Data/classifiers_rforest.pk.gz', 'rb') as f:\n",
    "    models = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with gzip.open('./Data/vectorizers.pk.gz', 'rb') as f:\n",
    "    vectorizers = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to read privacy policies from corpus\n",
    "Using the ACL Coling dataset from https://www.usableprivacy.org/static/data/acl-coling-2014-corpus.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def getSentencesFromXML(filename):\n",
    "\n",
    "    tree = ET.parse(filename)\n",
    " \n",
    "    # extract the website_url attribute\n",
    "    website = tree.find('.').attrib['website_url']\n",
    "    \n",
    "    # get a list of sentences from the privacy policy\n",
    "    sentences = []\n",
    "    for section in tree.getroot():\n",
    "        for subsection in section:\n",
    "            try:\n",
    "                blob = TextBlob(subsection.text)\n",
    "            except TypeError:\n",
    "                continue # ignore\n",
    "            for s in blob.sentences:\n",
    "                sentences.append(str(s))\n",
    "                \n",
    "    return website, sentences\n",
    "\n",
    "\n",
    "def classifySentences(vectorizers, models, sentences):\n",
    "    \n",
    "    classifiedSentences = {}\n",
    "    \n",
    "    for category in models:\n",
    "        vectorizer = vectorizers[category]\n",
    "        model = models[category]\n",
    "\n",
    "        X = vectorizer.transform(sentences)\n",
    "        predictions = model.predict(X)\n",
    "        \n",
    "        classifiedSentences[category] = []\n",
    "        \n",
    "        for i, predict in enumerate(predictions):\n",
    "            if predict == 1:\n",
    "                classifiedSentences[category].append(sentences[i])\n",
    "\n",
    "    return classifiedSentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through all files and classify each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "998 files processed\n",
      "\n",
      "CPU times: user 13min 8s, sys: 34.8 s, total: 13min 43s\n",
      "Wall time: 13min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "filenames = glob('./Data/corpus/*.xml')\n",
    "\n",
    "corpus_sentences = {}\n",
    "\n",
    "for filename in filenames:\n",
    "    website, sentences = getSentencesFromXML(filename)\n",
    "    corpus_sentences[website] = classifySentences(vectorizers, models, sentences)\n",
    "    \n",
    "print (\"{} files processed\\n\".format(len(corpus_sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out an example of how to access a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We take the security of your personal information seriously and use appropriate technical and organizational measures to protect your personal information against unauthorized or unlawful processing and against accidental loss, destruction or damage.\n"
     ]
    }
   ],
   "source": [
    "print (corpus_sentences['ask.com']['Data Security'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write corpus_sentences to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./Data/corpus_sentences.pk', 'wb') as f:\n",
    "    pickle.dump(corpus_sentences, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
